version: '3.0'
services:
  etcd:
    image: bitnami/etcd:3.4.3
    container_name: etcd
    hostname: etcd
    environment:
      ETCD_ENABLE_V2: 'true'
      ALLOW_NONE_AUTHENTICATION: 'yes'
      ETCD_LISTEN_PEER_URLS: http://0.0.0.0:2380
      ETCD_INITIAL_ADVERTISE_PEER_URLS: http://etcd:2380
      ETCD_LISTEN_CLIENT_URLS: http://0.0.0.0:2379
      ETCD_ADVERTISE_CLIENT_URLS: http://etcd:2379
      ETCD_INITIAL_CLUSTER: etcd=http://etcd:2380
      ETCD_INITIAL_CLUSTER_STATE: new
      ETCD_INITIAL_CLUSTER_TOKEN: pgEtcdCluster
    command: etcd -name etcd

  node-1: &spilo
    image: registry.opensource.zalan.do/acid/spilo-14:2.1-p7
    hostname: node-1
    container_name: node-1
    ports:
    # Port 2379-2380  etcd
    # Port 5432       Postgres
    # Port 8008       Patroni
    - "2379:2379"
    - "2380:2380"
    - "5432:5432"
    - "8008:8008"
    # Please use volume as opposed to tmpfs for production system.
    tmpfs: /home/postgres/pgdata
    depends_on:
      - etcd
    environment:
      ETCDCTL_ENDPOINTS: http://etcd:2379
      ETCD3_HOST: "etcd:2379"
      # A password for the superuser.
      PGPASSWORD_SUPERUSER: 'password'
      SCOPE: 'test-cluster'
      PGVERSION: 13
      USE_ADMIN: 'false'
      # Region of S3 bucket
      AWS_REGION: '<REGION>'
      # It should be in format 'https: //s3.AWS_REGION.amazonaws.com: 443'
      AWS_ENDPOINT: 'https: //s3.eu-west-1.amazonaws.com: 443/'
      # Conditional parameters AWS_ACCESS_KEY_ID & AWS_SECRET_ACCESS_KEY
      # Only relevant when you are running it locally.
      # On AWS cloud, you don't need to specify it. IAM profile takes care of it.
      AWS_ACCESS_KEY_ID: '<AWS_ACCESS_KEY_ID>'
      AWS_SECRET_ACCESS_KEY: '<AWS_SECRET_ACCESS_KEY>'
      # AWS S3 bucket name for storing backups
      WAL_S3_BUCKET: '<BUCKET_NAME_HERE>'
      # The full path to the backup location on AWS S3
      WALE_S3_PREFIX: 's3: //<BUCKET_NAME_HERE>/<BUCKET_OBJECT_HERE>/'
      # It should be in format 'https+path: //s3.AWS_REGION.amazonaws.com: 443'
      WALE_S3_ENDPOINT: 'https+path: //s3.eu-west-1.amazonaws.com: 443/'
      # Spilo supports wal-e as well as wal-g for backups. Our system uses wal-g since wal-e is obsolete.
      # Enforce using wal-g instead of wal-e for restores
      USE_WALG_RESTORE: 'true'
      # Enforce using wal-g instead of wal-e for backups
      USE_WALG_BACKUP: 'true'
      # To configure how many goroutines to use during backup-fetch and wal-fetch, use WALG_DOWNLOAD_CONCURRENCY.
      # By default, WAL-G uses the minimum of the number of files to extract and 10.
      WALG_DOWNLOAD_CONCURRENCY: 10
      # To configure how many concurrency streams to use during backup uploading, use WALG_UPLOAD_CONCURRENCY.
      # By default, WAL-G uses 16 streams.
      WALG_UPLOAD_CONCURRENCY: 16
      # If this setting is specified, during wal-push WAL-G will check the existence of WAL before uploading it.
      # If the different file is already archived under the same name, WAL-G will return the non-zero exit code
      # to prevent PostgreSQL from removing WAL.
      WALG_PREVENT_WAL_OVERWRITE: 'true'
      # To configure how many concurrency streams are reading disk during backup-push. By default, WAL-G uses 1 stream.
      WALG_UPLOAD_DISK_CONCURRENCY: 1
      # Maximum size of the WAL segments accumulated after the base backup to
      # consider WAL-G restore instead of pg_basebackup.
      WALE_BACKUP_THRESHOLD_MEGABYTES: 102400
      # Maximum ratio (in percents) of the accumulated WAL files to the
      # base backup to consider WAL-G restore instead of pg_basebackup.
      WALE_BACKUP_THRESHOLD_PERCENTAGE: 30
      # cron schedule for doing backups via WAL-G
      BACKUP_SCHEDULE: '00 01 * * *'
  node-2:
    <<: *spilo
    hostname: node-2
    container_name: node-2
    ports:
      - "5431:5432"
    depends_on:
      - node-1
  restore-node-from-backup:
    image: registry.opensource.zalan.do/acid/spilo-14:2.1-p7
    hostname: node-3
    container_name: node-3
    ports:
      # Port 2379-2380  etcd
      # Port 5432       Postgres
      # Port 8008       Patroni
      - "2379:2379"
      - "2380:2380"
      - "5432:5432"
      - "8008:8008"
    # Please use volume as opposed to tmpfs for production system.
    tmpfs: /home/postgres/pgdata
    environment:
      SCOPE: 'test-cluster'
      PGVERSION: 13
      USE_ADMIN: 'false'
      # A password for the superuser.
      # Please use the same password as that of the one used while backing up.
      PGPASSWORD_SUPERUSER: '<PASSWORD_HERE>'
      CLONE_WAL_S3_BUCKET: '<BUCKET_NAME_HERE>'
      CLONE_WALE_S3_PREFIX: 's3://<BUCKET_NAME_HERE>/<BUCKET_OBJECT_HERE>/'
      CLONE_WALE_S3_ENDPOINT: 'https+path://s3.AWS_REGION.amazonaws.com:443'
      CLONE_AWS_REGION: '<AWS_REGION>'
      CLONE_AWS_ENDPOINT: 'https://s3.AWS_REGION.amazonaws.com/'
      # Conditional parameters CLONE_AWS_ACCESS_KEY_ID & CLONE_AWS_SECRET_ACCESS_KEY
      # Only relevant when you are running it locally.
      # On AWS cloud, you don't need to specify it. IAM profile takes care of it.
      CLONE_AWS_ACCESS_KEY_ID: '<AWS_ACCESS_KEY_ID>'
      CLONE_AWS_SECRET_ACCESS_KEY: '<AWS_SECRET_ACCESS_KEY>'
      CLONE_USE_WALG_RESTORE: 'true'
      # Even though we are cloning but spilo need this env variable to be true.
      # Otherwise, it fails to detect correct AWS region.
      CLONE_USE_WALG_BACKUP: 'true'
      CLONE_WALG_DOWNLOAD_CONCURRENCY: 10
      # Please note that our system uses wal-g. So, using "CLONE_WITH_WALE" does not mean
      # that it uses wal-e. It is still using wal-g.
      CLONE_METHOD: 'CLONE_WITH_WALE'
      # Ignore the following setting if you want to recover from the latest backup.
      # (Optional) when you would like to restore backup at given timestamp,
      # set the time stamp up to which recovery will proceed.
      # Please specify it as `YYYY-mm-dd HH:mm:ss+00:00`.
      #
      # It's important to specify timezone. You can read more on this topic here [1]
      # [1] https://postgresqlco.nf/doc/en/param/recovery_target_time/
      CLONE_TARGET_TIME: '<timestamp-with-timezone>'